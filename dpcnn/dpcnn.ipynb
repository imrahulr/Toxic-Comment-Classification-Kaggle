{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already up-to-date: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2195895 word vectors of glove.\n",
      "-0.01444638 0.47249147\n",
      "Total 2195895 word vectors.\n",
      "Processing text dataset\n",
      "Found 292462 unique tokens\n",
      "Shape of data tensor: (159571, 200)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 200)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 200)\n"
     ]
    }
   ],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 21603\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def get_model():\n",
    "    filter_nr = 64\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.4\n",
    "    dense_dropout = 0.4\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    inp_post = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    emb_comment1 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    emb_comment1 = SpatialDropout1D(spatial_dropout)(emb_comment1)\n",
    "\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment1)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block11)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "\n",
    "    resize_emb1 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment1)\n",
    "    resize_emb1 = PReLU()(resize_emb1)\n",
    "\n",
    "    block1_output1 = add([block11, resize_emb1])\n",
    "    block1_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output1)\n",
    "\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output1)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block21)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "\n",
    "    block2_output1 = add([block21, block1_output1])\n",
    "    block2_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output1)\n",
    "\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output1)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block31)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "\n",
    "    block3_output1 = add([block31, block2_output1])\n",
    "    block3_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output1)\n",
    "\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output1)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block41)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "\n",
    "    output1 = add([block41, block3_output1])\n",
    "    output1 = GlobalMaxPooling1D()(output1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emb_comment2 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inp_post)\n",
    "    emb_comment2 = SpatialDropout1D(spatial_dropout)(emb_comment2)\n",
    "\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment2)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block12)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "\n",
    "    resize_emb2 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment2)\n",
    "    resize_emb2 = PReLU()(resize_emb2)\n",
    "\n",
    "    block1_output2 = add([block12, resize_emb2])\n",
    "    block1_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output2)\n",
    "\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output2)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block22)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "\n",
    "    block2_output2 = add([block22, block1_output2])\n",
    "    block2_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output2)\n",
    "\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output2)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block32)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "\n",
    "    block3_output2 = add([block32, block2_output2])\n",
    "    block3_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output2)\n",
    "\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output2)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block42)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "\n",
    "    output2 = add([block42, block3_output2])\n",
    "    output2 = GlobalMaxPooling1D()(output2)\n",
    "\n",
    "            \n",
    "    output = concatenate([output1, output2])\n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(6, activation='sigmoid')(output)\n",
    "   \n",
    "    model = Model(inputs=[inp, inp_post], outputs=output, name=\"DPCNN\")\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=1e-3,decay=0),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 200, 300)     30000000    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 200, 300)     30000000    input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 200, 300)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, 200, 300)     0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 200, 64)      57664       spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 200, 64)      57664       spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 200, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 200, 64)      256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_58 (PReLU)              (None, 200, 64)      12800       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_67 (PReLU)              (None, 200, 64)      12800       batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 200, 64)      12352       p_re_lu_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 200, 64)      12352       p_re_lu_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 200, 64)      256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 200, 64)      19264       spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 200, 64)      256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 200, 64)      19264       spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_59 (PReLU)              (None, 200, 64)      12800       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_60 (PReLU)              (None, 200, 64)      12800       conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_68 (PReLU)              (None, 200, 64)      12800       batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_69 (PReLU)              (None, 200, 64)      12800       conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 200, 64)      0           p_re_lu_59[0][0]                 \n",
      "                                                                 p_re_lu_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 200, 64)      0           p_re_lu_68[0][0]                 \n",
      "                                                                 p_re_lu_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 99, 64)       0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 99, 64)       0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 99, 64)       12352       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 99, 64)       12352       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 99, 64)       256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 99, 64)       256         conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_61 (PReLU)              (None, 99, 64)       6336        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_70 (PReLU)              (None, 99, 64)       6336        batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 99, 64)       12352       p_re_lu_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 99, 64)       12352       p_re_lu_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 99, 64)       256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 99, 64)       256         conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_62 (PReLU)              (None, 99, 64)       6336        batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_71 (PReLU)              (None, 99, 64)       6336        batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 99, 64)       0           p_re_lu_62[0][0]                 \n",
      "                                                                 max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 99, 64)       0           p_re_lu_71[0][0]                 \n",
      "                                                                 max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 49, 64)       0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 49, 64)       0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 49, 64)       12352       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 49, 64)       12352       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 49, 64)       256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 49, 64)       256         conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_63 (PReLU)              (None, 49, 64)       3136        batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_72 (PReLU)              (None, 49, 64)       3136        batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 49, 64)       12352       p_re_lu_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 49, 64)       12352       p_re_lu_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 49, 64)       256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 49, 64)       256         conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_64 (PReLU)              (None, 49, 64)       3136        batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_73 (PReLU)              (None, 49, 64)       3136        batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 49, 64)       0           p_re_lu_64[0][0]                 \n",
      "                                                                 max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 49, 64)       0           p_re_lu_73[0][0]                 \n",
      "                                                                 max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 24, 64)       0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 24, 64)       0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 24, 64)       12352       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 24, 64)       12352       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 24, 64)       256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 24, 64)       256         conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_65 (PReLU)              (None, 24, 64)       1536        batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_74 (PReLU)              (None, 24, 64)       1536        batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 24, 64)       12352       p_re_lu_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 24, 64)       12352       p_re_lu_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 24, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 24, 64)       256         conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_66 (PReLU)              (None, 24, 64)       1536        batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_75 (PReLU)              (None, 24, 64)       1536        batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 24, 64)       0           p_re_lu_66[0][0]                 \n",
      "                                                                 max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 24, 64)       0           p_re_lu_75[0][0]                 \n",
      "                                                                 max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          33024       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 256)          1024        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_76 (PReLU)              (None, 256)          256         batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           p_re_lu_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            1542        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 60,487,558\n",
      "Trainable params: 484,998\n",
      "Non-trainable params: 60,002,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data, data_post, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"dpcnn_fold{0}.h5\".format(fold_id)\n",
    "        model = get_model()\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp], train_y, epochs=15, batch_size=128, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting validation...\")\n",
    "        val_predicts_path = \"dpcnn_val_predicts{0}.npy\".format(fold_id)\n",
    "        val_predicts = model.predict([val_x, val_xp], batch_size=1024, verbose=1)\n",
    "        np.save(val_predicts_path, val_predicts)\n",
    "        \n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"dpcnn_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Fold 9\n",
      "dpcnn_fold9.h5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 126s 880us/step - loss: 0.0408 - acc: 0.9839 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04295, saving model to dpcnn_fold9.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 123s 857us/step - loss: 0.0393 - acc: 0.9843 - val_loss: 0.0425 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04295 to 0.04249, saving model to dpcnn_fold9.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 122s 852us/step - loss: 0.0378 - acc: 0.9848 - val_loss: 0.0445 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 123s 856us/step - loss: 0.0364 - acc: 0.9853 - val_loss: 0.0415 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04249 to 0.04152, saving model to dpcnn_fold9.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 123s 854us/step - loss: 0.0350 - acc: 0.9856 - val_loss: 0.0436 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 123s 856us/step - loss: 0.0341 - acc: 0.9859 - val_loss: 0.0425 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 123s 855us/step - loss: 0.0328 - acc: 0.9864 - val_loss: 0.0439 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Fold 9 loss 0.04151918721057467\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 5s 332us/step\n",
      "Predicting results...\n",
      "148480/153164 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val 0.04225\n",
    "\n",
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_dpcnn_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv(\"10fold_dpcnn_gm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neptune",
   "language": "",
   "name": "neptune-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
